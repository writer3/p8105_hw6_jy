---
title: "HW 6"
output: github_document
---

```{r}
library(tidyverse)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


Load key packages

```{r}
library(p8105.datasets)
library(modelr)
library(purrr)
```

## Problem 2

Creating a `city_state` variable (e.g. “Baltimore, MD”), and a binary variable called `resolution` indicating whether the homicide is solved. Omitting cities Dallas, TX; Phoenix, AZ; and Kansas City, MO as these don’t report victim race. Also omiting Tulsa, AL as this is a data entry mistake. For this problem, limiting the analysis those for whom victim_race is white or black. Ensuring that victim_age is numeric.

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Open/No arrest" ~ 0,
      disposition == "Closed without arrest" ~ 0,
      disposition == "Closed by arrest" ~ 1)
    ) |> 
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))) |> filter(victim_race %in% c("White", "Black"))
```


For Baltimore, MD, using the `glm` function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. Saving the output of `glm` as an R object; applying the `broom::tidy` to this object; and obtaining the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.

```{r}
balt_glm =
  homicide_df |> 
  filter(city_state == "Baltimore, MD") |> 
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)

balt_glm |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate),
    OR_CI_upper = exp(estimate + 1.96 * std.error), 
    OR_CI_lower = exp(estimate - 1.96 * std.error)
  ) |> 
  filter(term == "victim_sexMale") |> 
  knitr::kable(digits = 3)
```


Running `glm` for each of the cities in the dataset, and extracting the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. Doing this within a “tidy” pipeline, making use of `purrr::map`, list columns, and unnesting as necessary to create a dataframe with estimated ORs and CIs for each city.


```{r}
mod_results =
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    mods = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race,
                                 family = binomial(), data = df)),
    tidy_mods = map(mods, broom::tidy)) |> 
  select(-mods, -data) |> 
  unnest(cols = tidy_mods) |> 
  mutate(
    OR = exp(estimate),
    OR_CI_upper = exp(estimate + 1.96 * std.error), 
    OR_CI_lower = exp(estimate - 1.96 * std.error)
  ) |> 
  filter(term == "victim_sexMale")

mod_results |> 
  slice(1:3)|> 
  knitr::kable(digits = 3)
```


Creating a plot that shows the estimated ORs and CIs for each city. Organizing cities according to estimated OR. 


```{r}
mod_results |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


This plot shows that many cities have OR's that include 1 within the confidence intervals, meaning that the OR estimates are not statistically significant. Some cities, such as New York, NY, Baton Rouge, LA, and Baltimore, MD, have OR's less than 1 and the confidence intervals do not include 1, meaning that the crimes with victims that are male have smaller odds of resolution compared to that of female victims adjusting for age and race, and that this is statistically significant for these cities. 



## Problem 3

Loading and cleaning the data for regression analysis. Converting numeric to factor where appropriate and checking for missing data.

```{r}
birthweight_df =
  read_csv("data/birthweight.csv") |> 
  janitor::clean_names() |>
  drop_na() |> 
  mutate(
    babysex = factor(case_match(babysex, 1 ~ "male", 2 ~ "female")),
    frace = factor(case_match(frace, 
                              1 ~ "white", 
                              2 ~ "black", 
                              3 ~ "asian", 
                              4 ~ "puerto rican", 
                              8 ~ "other", 
                              9 ~ "unknown")),
    malform = factor(case_match(malform, 0 ~ "absent", 1 ~ "present")),
    mrace = factor(case_match(mrace, 
                              1 ~ "white", 
                              2 ~ "black", 
                              3 ~ "asian", 
                              4 ~ "puerto rican", 
                              8 ~ "other", 
                              9 ~ "unknown"))
  )
```


Building a linear regression model for birthweight on a data-driven modeling-building process.

```{r}
lin_mod = lm(bwt ~ ., data = birthweight_df)
fin_mod = step(lin_mod, trace = 1)

summary(fin_mod)
```

The initial linear regression model was fit with `bwt` as the dependent variable and `.` or all other columns in the dataset as independent variables. Then `step` function was used to perform a stepwise model selection to refine the model. The default method, or the backward elimination, was used.

Plotting the model residuals against fitted values

```{r}
birthweight_df |> 
  add_predictions(fin_mod) |> 
  add_residuals(fin_mod) |> 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .5) +
  geom_smooth(se = FALSE) +
  labs(title = "Model Residuals Against Fitted Values",
       x = "Fitted Values",
       y = "Residuals")
```


Making two models for comparison: first one using length at birth and gestational age as predictors (main effects only), and the second one using head circumference, length, sex, and all interactions (including the three-way interaction) between these. 

```{r}
length_age_mod = lm(bwt ~ blength + gaweeks, data = birthweight_df)

summary(length_age_mod)



head_full_mod = lm(bwt ~ bhead * blength * babysex, data = birthweight_df)

summary(head_full_mod)
```


Comparing my model to two others Using `crossv_mc` and functions in `purrr` as appropriate. 

Using `crossv_mc` to create training and testing sets, then converting to tibbles.

```{r}
cv_df =
  crossv_mc(birthweight_df, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```


Applying each model to the training set dataframe and evaluating all testing dataframe using `rmse`.

```{r}
cv_result =
  cv_df |> 
  mutate(
    fin_model = map(train, ~fin_mod),
    length_age_model = map(train, ~length_age_mod),
    head_full_model = map(train, ~head_full_mod),
  ) |> 
  mutate(
    rmse_fin_mod = map2_dbl(fin_model, 
                            test, \(mod, df) rmse(model = mod, data = df)),
    rmse_length_age_mod = map2_dbl(length_age_model, 
                                   test, \(mod, df) rmse(model = mod, data = df)),
    rmse_head_full_mod = map2_dbl(head_full_model, 
                                   test, \(mod, df) rmse(model = mod, data = df))
  )

cv_result |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin()
```

The violin plot shows that the rms of my model is less than the other two models. This means that my model performs better than the other two models given that it has the smallest `rmse`. 
